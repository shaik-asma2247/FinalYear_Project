{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets loading\n",
    "\n",
    "In this section we load the datasets and, since the dataset is too big, we take a sample of it if in development mode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importing the basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Importing machine learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load machine learning libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Development mode flag\n",
    "\n",
    "If the following flag is set to `True`, the model will be trained on a smaller dataset, in order to speed up the development process. If the flag is set to `False`, the model will be trained on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If true, only the 3% of the data will be used for training and testing of the various models\n",
    "_DEVMODE = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the train and test files\n",
    "train_df = pd.read_csv('data/train_net.csv')\n",
    "test_df = pd.read_csv('data/test_net.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Loaded datasets information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total size\n",
    "print(\"Test set size: \", test_df.shape)\n",
    "print(\"Train set size: \", train_df.shape)\n",
    "\n",
    "# Value counts\n",
    "train_df['ALERT'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Dataset development mode reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _DEVMODE:\n",
    "    train_df = train_df.sample(frac=0.03, random_state=1)\n",
    "    test_df = test_df.sample(frac=0.03, random_state=1)\n",
    "\n",
    "    # Print total size\n",
    "    print(\"Test set size: \", test_df.shape)\n",
    "    print(\"Train set size: \", train_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "\n",
    "In this section we preprocess the datasets in order to make them usable by the machine learning algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Print datasets information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Print datasets shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show information about the data\n",
    "def printInfo(df):\n",
    "    print('Dataframe shape: ', df.shape)\n",
    "    print('Dataframe columns: ', df.columns)\n",
    "\n",
    "print('==== Train data ====')\n",
    "printInfo(train_df)\n",
    "print()\n",
    "print('==== Test data ====')\n",
    "printInfo(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Show training dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('==== Train data ====')\n",
    "print(train_df.isnull().sum())\n",
    "print()\n",
    "print('==== Test data ====')\n",
    "print(test_df.isnull().sum())\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Fill missing **ANOMALY** values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing ANOMALY values with 0 (no anomaly)\n",
    "train_df['ANOMALY'] = train_df['ANOMALY'].fillna(0)\n",
    "test_df['ANOMALY'] = test_df['ANOMALY'].fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data analysis\n",
    "\n",
    "In this section we analyze the datasets in order to have a better understanding of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Observing the distribution of the target variable\n",
    "\n",
    "We can observe that the dataset is highly imbalanced, with the majority of the flows being normal (no attack detected). We can also observe that also the number of malware attacks is very low, compared to the other attacks.\n",
    "\n",
    "These two facts will have a big impact on the model training, as we will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of the target variable\n",
    "sns.countplot(x='ALERT', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique protocol_maps\n",
    "train_df['PROTOCOL_MAP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# seaborn countplots\n",
    "sns.countplot(x='ANOMALY', data=train_df, ax=axs[0]).set(title='ANOMALY')\n",
    "              \n",
    "\n",
    "# Seaborn countplot for the 'PROTOCOL_MAP' column, with enough space for the labels\n",
    "sns.countplot(x='PROTOCOL_MAP', data=train_df, ax=axs[1]).set(title='PROTOCOL_MAP')\n",
    "\n",
    "# Boxplot for L4_SRC_PORT to undestand the distribution of the data\n",
    "sns.boxplot(\n",
    "    x='L4_SRC_PORT', data=train_df, ax=axs[2],\n",
    "    notch=True, showcaps=True,\n",
    "    flierprops={\"marker\": \"x\"}, # Change the outlier marker\n",
    "    showmeans=True, # Show the mean\n",
    "    boxprops={\"facecolor\": (.4, .6, .8, .5)},\n",
    "  ).set(title='L4_SRC_PORT')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Protocol distribution in relation to the kind of attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show protocol_map distribution for kind of ALERT\n",
    "sns.countplot(x='PROTOCOL_MAP', hue='ALERT', data=train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Unique hosts in dataset\n",
    "\n",
    "Knowing the amount of unique hosts in the dataset is important to understand the size of the dataset since I expect that a bigger dataset will be more difficult to train properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique hosts (IP addresses) in the train and test data\n",
    "train_src_hosts = train_df['IPV4_SRC_ADDR'].unique()\n",
    "train_dst_hosts = train_df['IPV4_DST_ADDR'].unique()\n",
    "train_hosts = np.union1d(train_src_hosts, train_dst_hosts)\n",
    "\n",
    "# For each host, count the number of flows\n",
    "print('Number of unique hosts in the train data: ', len(train_hosts))\n",
    "\n",
    "# Find unique hosts (IP addresses) in the train and test data\n",
    "test_src_hosts = test_df['IPV4_SRC_ADDR'].unique()\n",
    "test_dst_hosts = test_df['IPV4_DST_ADDR'].unique()\n",
    "test_hosts = np.union1d(test_src_hosts, test_dst_hosts)\n",
    "\n",
    "# Floor ratio of hosts in test data that are not in train data\n",
    "ratio = math.floor((1.0-len(test_hosts)/len(train_hosts)) * 100)\n",
    "\n",
    "# For each host, count the number of flows\n",
    "print(\"Number of unique hosts in the test data: {} (~{}% smaller)\".format(len(test_hosts), ratio))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Distribution analysis using pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns to be used for training\n",
    "train_df_columns = train_df[['L4_SRC_PORT', 'L4_DST_PORT', 'PROTOCOL', 'ANOMALY', 'ALERT']]\n",
    "\n",
    "# Distribution analysis using pairplot\n",
    "sns.pairplot(train_df_columns, hue='ALERT')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Remove useless columns and create dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revoked columns\n",
    "revoked_columns = [\n",
    "  'FLOW_ID', # Completely random\n",
    "  'ID', # Completely random\n",
    "  'ANALYSIS_TIMESTAMP', # Completely random\n",
    "  'IPV4_SRC_ADDR', # Not useful for the model\n",
    "  'IPV4_DST_ADDR', # Not useful for the model\n",
    "  'PROTOCOL_MAP', # There is a numerical column for the protocol\n",
    "  'MIN_IP_PKT_LEN', # Always 0 since it is a minimum value\n",
    "  'MAX_IP_PKT_LEN', # Always 0 (maybe it means that the packet have infinite length?)\n",
    "  'TOTAL_PKTS_EXP', # Always 0\n",
    "  'TOTAL_BYTES_EXP', # Always 0\n",
    "]\n",
    "\n",
    "# Create dummy columns for the ALERT column\n",
    "alert_dummies = pd.get_dummies(train_df['ALERT'], prefix='ALERT', drop_first=True)\n",
    "\n",
    "# Copy + drop the revoked columns\n",
    "train_df = train_df.copy().drop(revoked_columns, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Correlation heatmap\n",
    "\n",
    "We can observe that there are some features that are highly correlated with each other, such as **IN_BYTES** - **OUT_BYTES** and **IN_PKTS** - **OUT_PKTS**. This is not surprising, since these features are related to the amount of data exchanged between the two hosts.\n",
    "\n",
    "We can also observe that a *port scanning* alert is highly correlated with the **L4_DST_PORT** and **ANOMALY** features. This is not surprising, since a port scanning attack is a type of attack that tries to find open ports on a host. It is highly correlated with **ANOMALY** probably because the forged packets are built in a way that they are not recognized as an attack by the network.\n",
    "\n",
    "Unfortunately, since *malware attacks* alerts are various and have different characteristics/features, it is not possible to find a correlation between them and the other features. This could mean that the features used in this dataset are not enough to detect malware attacks.\n",
    "\n",
    "In the other hand, *none* alerts are strongly negatively correlated with **ANOMALY** and **L4_DST_PORT**. This is not surprising, since a normally a flow contains valid packets and the destination is usually a well-known port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap using pandas\n",
    "corr = pd.concat([train_df.drop('ALERT', axis=1), alert_dummies], axis=1).corr(\n",
    "  numeric_only=False, # Only consider numeric columns\n",
    ")\n",
    "\n",
    "# Correlation heatmap using seaborn + make annotations fit the heatmap\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(corr, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset preparation\n",
    "\n",
    "In this section we prepare the dataset for the machine learning algorithms. We will split the dataset into training and testing sets, and we will also scale the data to make it more suitable for the algorithms.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Splitting the training set\n",
    "\n",
    "Since we already have a test set, we split our training set in training and validation sets. We will use Sklearn's `StratifiedShuffleSplit` to split the training set in 80% training and 20% validation maintaining the same distribution of the target variable. This is needed since the dataset is highly imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_maintain_distribution(X, y):\n",
    "  sss=StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=9)\n",
    "  indexes = sss.split(X, y)\n",
    "  train_indices, test_indices = next(indexes)\n",
    "  return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'ALERT' is NaN\n",
    "train_df_cleaned = train_df.dropna(subset=['ALERT'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = split_maintain_distribution(\n",
    "    train_df_cleaned.drop('ALERT', axis=1), \n",
    "    train_df_cleaned['ALERT']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['ALERT'].isna().sum())  # Number of missing values in target\n",
    "print(train_df['ALERT'].value_counts(dropna=False))  # Show class distribution including NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def split_maintain_distribution(X, y):\n",
    "    if y.isna().any():\n",
    "        raise ValueError(\"Target variable 'y' contains NaN values. Clean your data before splitting.\")\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=9)\n",
    "    indexes = sss.split(X, y)\n",
    "    train_indices, test_indices = next(indexes)\n",
    "    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Step 1: Fill NaNs in 'ALERT' with a placeholder label (e.g., 'Unknown')\n",
    "train_df['ALERT'] = train_df['ALERT'].fillna('Unknown')\n",
    "\n",
    "# Step 2: Convert all labels in 'ALERT' to string type to ensure uniformity\n",
    "train_df['ALERT'] = train_df['ALERT'].astype(str)\n",
    "\n",
    "# Step 3: Define the function to split maintaining label distribution\n",
    "def split_maintain_distribution(X, y):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=9)\n",
    "    indexes = sss.split(X, y)\n",
    "    train_indices, test_indices = next(indexes)\n",
    "    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "# Step 4: Perform the split\n",
    "X_train, X_val, y_train, y_val = split_maintain_distribution(\n",
    "    train_df.drop('ALERT', axis=1), \n",
    "    train_df['ALERT']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['ALERT'].apply(type).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = split_maintain_distribution(train_df.drop('ALERT', axis=1), train_df['ALERT'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check if actually the distribution of the target variable is the same in the training and validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Check if the datasets are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print distribution of the target variable in the train and validation sets\n",
    "print('Train set distribution:')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print()\n",
    "print('Validation set distribution:')\n",
    "print(y_val.value_counts(normalize=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the distribution of the target variable is the same in the training and validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Data scaling\n",
    "\n",
    "Scaling the data is important to avoid that some features will have a bigger impact on the model training than others. This is especially important when we are dealing with features that have different units of measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix scaler on train set\n",
    "scaler = StandardScaler()\n",
    "fitter = scaler.fit(X_train)\n",
    "\n",
    "# Scale train and validation sets\n",
    "x_train_scaled = fitter.transform(X_train)\n",
    "x_validation_scaled = fitter.transform(X_val)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "df_feat_train = pd.DataFrame(x_train_scaled, columns=X_train.columns)\n",
    "df_feat_validation = pd.DataFrame(x_validation_scaled, columns=X_val.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature selection\n",
    "\n",
    "In this section we will use a Random Forest classifier to find the most important features in the dataset. This will help us to reduce the number of features used in the model training, and therefore speed up the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Create model and fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100) # 100 trees = default value\n",
    "\n",
    "# Fit the model\n",
    "rfc.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Get feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print features importance\n",
    "feature_importances = pd.DataFrame(\n",
    "    rfc.feature_importances_,\n",
    "    index=X_train.columns,\n",
    "    columns=['importance']\n",
    ").sort_values('importance', ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Plot feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=-90)\n",
    "sns.barplot(x=feature_importances.index, y=feature_importances['importance'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Select most important features\n",
    "\n",
    "Select the most important features using the Random Forest classifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_IMPORTANCE_THRESHOLD = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns with importance > 0.02\n",
    "COLUMNS = feature_importances[feature_importances['importance'] > MIN_IMPORTANCE_THRESHOLD].index\n",
    "COLUMNS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Reprepare the dataset with the selected features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1. Split again the training set into training and validation sets (with new features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = split_maintain_distribution(\n",
    "  train_df[COLUMNS],\n",
    "  train_df['ALERT']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.2. Scale again the train and validation sets (with new features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix scaler on train set\n",
    "scaler = StandardScaler()\n",
    "fitter = scaler.fit(X_train)\n",
    "\n",
    "# Scale train and validation sets\n",
    "x_train_scaled = fitter.transform(X_train)\n",
    "x_validation_scaled = fitter.transform(X_val)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "df_feat_train = pd.DataFrame(x_train_scaled, columns=X_train.columns)\n",
    "df_feat_validation = pd.DataFrame(x_validation_scaled, columns=X_val.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.3. Scale also the test set (with new features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No target variable, so no need to split the fit and transform\n",
    "x_test_scaled = StandardScaler().fit_transform(test_df[COLUMNS])\n",
    "# Convert to pandas dataframe\n",
    "df_feat_test = pd.DataFrame(x_test_scaled, columns=test_df[COLUMNS].columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. UMAP visualization\n",
    "\n",
    "In this section we will use UMAP to visualize the dataset in 2D. This will help us to understand if is possible to separate the different classes of attacks in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Create UMAP model and fit it using training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(\n",
    "  random_state=42,\n",
    "  n_neighbors=50,\n",
    "  min_dist=0.3,\n",
    ")\n",
    "mapper = reducer.fit(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(\n",
    "    n_neighbors=50,\n",
    "    min_dist=0.3,\n",
    "    n_jobs=-1  # use all available cores\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. UMAP visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1. Visualization using matplotlib\n",
    "\n",
    "Reduce data dimensionality to 2 dimensions and plot the data using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: assuming you have a features matrix called X_train\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding = reducer.fit_transform(X_train)  # X_train must be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define mapping dictionary\n",
    "label_map = {\n",
    "    'None': 0,\n",
    "    'Port Scanning': 1,\n",
    "    'Denial of Service': 2,\n",
    "    'Malware': 3\n",
    "}\n",
    "\n",
    "# 2. Fill NaNs with 'None' (or other default)\n",
    "y_train_filled = y_train.fillna('None')\n",
    "\n",
    "# 3. Map labels\n",
    "color_indices = y_train_filled.map(label_map)\n",
    "\n",
    "# 4. Check for unmapped labels (to debug)\n",
    "if color_indices.isnull().any():\n",
    "    print(\"Unmapped labels found:\", y_train_filled[color_indices.isnull()].unique())\n",
    "\n",
    "# 5. Convert to integer after mapping and fill unmapped with 0 (default to 'None' color)\n",
    "color_indices = color_indices.fillna(0).astype(int)\n",
    "\n",
    "# 6. Plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=[sns.color_palette()[x] for x in color_indices]\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the train set', fontsize=24)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2. Visualization using `umap.plot`\n",
    "Plot the data using `umap.plot` (which uses matplotlib under the hood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib datashader bokeh holoviews scikit-image colorcet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "\n",
    "# Create data labels\n",
    "labels = y_train.map({'None': 0, 'Port Scanning': 1, 'Denial of Service': 2, 'Malware': 3})\n",
    "# Visualize the embedding using umap.plot\n",
    "p = umap.plot.points(\n",
    "    mapper,\n",
    "    labels=y_train,\n",
    "    width=1000,\n",
    "    height=900,\n",
    ")\n",
    "umap.plot.show(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "\n",
    "In this section we will train different models and compare their results. We will use the following models:\n",
    "\n",
    "* K-Nearest Neighbors (KNN)\n",
    "* Support Vector Machine (SVM) with RBF kernel (Radial Basis Function)\n",
    "  * SVC\n",
    "  * SVC with PCA (Principal Component Analysis) pipeline\n",
    "* Bagging Classifier (SVC with RBF kernel)\n",
    "* Random Forest Classifier\n",
    "* Extra Trees Classifier\n",
    "* Neural Network (MLPClassifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. KNN Classifier training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the model is excessively precise, with a precision of 1.0 with any kind of attack. This is probably due to the fact that the dataset is highly imbalanced, with the majority of the flows being normal (no attack detected). We can also observe that also the number of malware attacks is very low, compared to the other attacks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Finding the best K hyperparameter for KNN\n",
    "\n",
    "To find the best K hyperparameter for KNN, we will use the **validation set** to find the best K value. We will then use this K value to train the model on the **training set** and evaluate it on the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best K using GridSearchCV\n",
    "MAX_DEGREE = 30\n",
    "\n",
    "k_range = list(range(1, MAX_DEGREE+1))\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "knn = KNeighborsClassifier()\n",
    "grid = GridSearchCV(knn, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Print information about the model\n",
    "print(f\"Best k: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(num=0, dpi=96, figsize=(10, 6))\n",
    "plt.plot(k_range, grid.cv_results_['mean_test_score'])\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.xticks(k_range)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the graphical outcome, the best parameter for KNN is **K = 1**. Since this value would lead to overfitting, we will use the first odd number after 1, which is **K = 3**.\n",
    "\n",
    "This outcome is not surprising since the training and validation sets are coming probably from the same network and the same hosts, so the flows are very similar to each other. This means that the best way to test our model is to use the **test set**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2. Fit model with best K hyperparameter + make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNN classifier with 3 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=3) # 3 = view note above\n",
    "# Fit the classifier to the data\n",
    "knn.fit(x_train_scaled, y_train)\n",
    "# Make predictions on validation set\n",
    "predictions = knn.predict(x_validation_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3. Model evaluation based on validation set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and index for the confusion matrix\n",
    "cmat = confusion_matrix(y_val, predictions)\n",
    "cmat = pd.DataFrame(cmat, index=['Denial of Service', 'Malware', 'None', 'Port Scan'], columns=['Denial of Service', 'Malware', 'None', 'Port Scan'])\n",
    "\n",
    "# Use seaborn to visualize the confusion matrix\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cmat, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.4. KNN predictions on test set\n",
    "\n",
    "Unfortunately, the test set doesn't include the target variable, so we can't evaluate the model on it. We can only evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - is None --no attack\n",
    "# Prediction on the test set\n",
    "predictions = knn.predict(x_test_scaled)\n",
    "\n",
    "# Show the predictions on a histogram\n",
    "fig = sns.countplot(x=predictions)\n",
    "fig.set_title('Predictions distribution on the test set') # Set the title\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=45) # Rotate x-labels\n",
    "pd.Series(predictions).value_counts() # Print the predictions size per class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Support Vector Machine Classifier (SVC) training\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges. However, it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1 Only SVC model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.1.1. Grid search to find best hyperparameters for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search parameters\n",
    "param_grid = {\n",
    "  'C': [0.1, 1, 10, 100, 1000],\n",
    "  'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "}\n",
    "\n",
    "# Create grid search\n",
    "svc_grid = GridSearchCV(\n",
    "  SVC(kernel=\"rbf\"),\n",
    "  param_grid,\n",
    "  cv=2, # Only 2 folds because of the size of the dataset, otherwise it takes too long\n",
    "  n_jobs=-1, # Use all cores\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "svc_grid.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Print information about the model\n",
    "print(f\"Best params: {svc_grid.best_params_}\")\n",
    "print(f\"Best score: {svc_grid.best_score_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.1.2. Create model with best parameters + fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVM with best parameters\n",
    "svc = SVC(\n",
    "  kernel='rbf',\n",
    "  C=svc_grid.best_params_['C'],\n",
    "  gamma=svc_grid.best_params_['gamma'],\n",
    ")\n",
    "svc.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.1.3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "predictions = svc.predict(x_validation_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.1.4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and index for the confusion matrix\n",
    "cmat = confusion_matrix(y_val, predictions)\n",
    "cmat = pd.DataFrame(cmat, index=['Denial of Service', 'Malware', 'None', 'Port Scan'], columns=['Denial of Service', 'Malware', 'None', 'Port Scan'])\n",
    "\n",
    "# Use seaborn to visualize the confusion matrix\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cmat, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.1.5. SVC model predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "predictions = svc.predict(x_test_scaled)\n",
    "\n",
    "# Show the predictions on a histogram\n",
    "fig = sns.countplot(x=predictions)\n",
    "fig.set_title('Predictions distribution on the test set') # Set the title\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=45) # Rotate x-labels\n",
    "pd.Series(predictions).value_counts() # Print the predictions size per class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2. PCA + SVC model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.2.1. Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two parameters\n",
    "pca = PCA(whiten=True, random_state=42) # PCA (Principal Component Analysis)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced') # SVC (Support Vector Classification)\n",
    "\n",
    "# Create pipeline\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.2.2. Grid search to find the best parameters for PCA and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a valid n_components range (from 5 to maximum number of features)\n",
    "n_features = x_train_scaled.shape[1]\n",
    "n_components = np.arange(5, n_features, 3)\n",
    "\n",
    "param_grid = {\n",
    "  'pca__n_components': n_components,\n",
    "  'svc__C': [50, 100, 500, 1000, 5000, 10000],\n",
    "  'svc__gamma': [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Grid search \n",
    "pipeline_grid = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    cv=2, # Only 2 folds because of the size of the dataset, otherwise it takes too long\n",
    "    n_jobs=-1 # Use all cores\n",
    ")\n",
    "pipeline_grid.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Print information about the model\n",
    "print(f\"Best params: {pipeline_grid.best_params_}\")\n",
    "print(f\"Best score: {pipeline_grid.best_score_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.2.3. Create pipeline with best parameters + fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create the desired pipeline\n",
    "pca = PCA(\n",
    "  n_components=pipeline_grid.best_params_['pca__n_components'],\n",
    "  whiten=True,\n",
    "  random_state=42\n",
    ")\n",
    "svc = SVC(kernel='rbf',\n",
    "  class_weight='balanced',\n",
    "  # Use the best parameters found by the grid search\n",
    "  C=pipeline_grid.best_params_['svc__C'],\n",
    "  gamma=pipeline_grid.best_params_['svc__gamma']\n",
    ")\n",
    "model = make_pipeline(pca, svc)\n",
    "model.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.2.4. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "predictions = model.predict(x_validation_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.2.5. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and index for the confusion matrix\n",
    "cmat = confusion_matrix(y_val, predictions)\n",
    "cmat = pd.DataFrame(cmat, index=['Denial of Service', 'Malware', 'None', 'Port Scan'], columns=['Denial of Service', 'Malware', 'None', 'Port Scan'])\n",
    "\n",
    "# Use seaborn to visualize the confusion matrix\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cmat, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.2.6. SVC+PCA pipeline model predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "predictions = model.predict(x_test_scaled)\n",
    "\n",
    "# Show the predictions on a histogram\n",
    "fig = sns.countplot(x=predictions)\n",
    "fig.set_title('Predictions distribution on the test set') # Set the title\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=45) # Rotate x-labels\n",
    "pd.Series(predictions).value_counts() # Print the predictions size per class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Bagging Classifier (SVC based) training\n",
    "\n",
    "Bagging Classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.1. Create model using best SVC parameters + fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf',\n",
    "  class_weight='balanced',\n",
    "  C=svc_grid.best_params_['C'],\n",
    "  gamma=svc_grid.best_params_['gamma']\n",
    ")\n",
    "\n",
    "clf = BaggingClassifier(\n",
    "  svc,\n",
    "  n_estimators=30,\n",
    "  n_jobs=-1, # Use all cores\n",
    "  random_state=42\n",
    ")\n",
    "clf.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(x_validation_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and index for the confusion matrix\n",
    "cmat = confusion_matrix(y_val, predictions)\n",
    "cmat = pd.DataFrame(cmat, index=['Denial of Service', 'Malware', 'None', 'Port Scan'], columns=['Denial of Service', 'Malware', 'None', 'Port Scan'])\n",
    "\n",
    "# Use seaborn to visualize the confusion matrix\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cmat, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.5. Bagging Classifier predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "predictions = clf.predict(x_test_scaled)\n",
    "\n",
    "# Show the predictions on a histogram\n",
    "fig = sns.countplot(x=predictions)\n",
    "fig.set_title('Predictions distribution on the test set') # Set the title\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=45) # Rotate x-labels\n",
    "pd.Series(predictions).value_counts() # Print the predictions size per class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Random Forest Classifier training\n",
    "\n",
    "Random Forest is an ensemble method that combines multiple decision trees to create a more accurate model. It is a supervised learning algorithm that can be used for both classification and regression tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.1. Grid search to find best hyperparameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Create a dictionary of all values we want to test for n_estimators\n",
    "parameters = {'n_estimators': [1, 2, 4, 10, 15, 20, 30, 40, 50, 100, 200, 500, 1000]}\n",
    "\n",
    "# Used to find the best n_estimators value to use to train the model\n",
    "rfc_grid = GridSearchCV(\n",
    "  rfc,\n",
    "  parameters,\n",
    "  scoring='accuracy',\n",
    "  cv=2, # Only 2 folds because of the size of the dataset, otherwise it takes too long\n",
    "  n_jobs=-1 # Use all cores\n",
    ")\n",
    "\n",
    "# Fit model to data\n",
    "rfc_grid.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Extract best params\n",
    "print(f\"Best params: {rfc_grid.best_params_}\")\n",
    "print(f\"Best score: {rfc_grid.best_score_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2. Create model with best parameters + fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=rfc_grid.best_params_['n_estimators'])\n",
    "rfc.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "predictions = rfc.predict(x_validation_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and index for the confusion matrix\n",
    "cmat = confusion_matrix(y_val, predictions)\n",
    "cmat = pd.DataFrame(cmat, index=['Denial of Service', 'Malware', 'None', 'Port Scan'], columns=['Denial of Service', 'Malware', 'None', 'Port Scan'])\n",
    "\n",
    "# Use seaborn to visualize the confusion matrix\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cmat, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.5. Random Forest model predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "predictions = rfc.predict(x_test_scaled)\n",
    "\n",
    "# Show the predictions on a histogram\n",
    "fig = sns.countplot(x=predictions)\n",
    "fig.set_title('Predictions distribution on the test set') # Set the title\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=45) # Rotate x-labels\n",
    "pd.Series(predictions).value_counts() # Print the predictions size per class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Extra Trees Classifier training\n",
    "\n",
    "This kind of classifier is an ensemble of decision trees. It is similar to a Random Forest classifier, but the trees are trained using the whole dataset instead of a bootstrap sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.1. Grid search to find best hyperparameters for Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest classifier\n",
    "etc = ExtraTreesClassifier()\n",
    "\n",
    "# Create a dictionary of all values we want to test for n_estimators\n",
    "parameters = {'n_estimators': [1, 2, 4, 10, 15, 20, 30, 40, 50, 100, 200, 500]}\n",
    "\n",
    "# Used to find the best n_estimators value to use to train the model\n",
    "etc_grid = GridSearchCV(\n",
    "  etc,\n",
    "  parameters,\n",
    "  scoring='accuracy',\n",
    "  cv=2, # Only 2 folds because of the size of the dataset, otherwise it takes too long\n",
    "  n_jobs=-1 # Use all cores\n",
    ")\n",
    "\n",
    "# Fit model to data\n",
    "etc_grid.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Extract best params\n",
    "print(f\"Best params: {etc_grid.best_params_}\")\n",
    "print(f\"Best score: {etc_grid.best_score_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.2. Create model with best parameters + fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier(n_estimators=etc_grid.best_params_['n_estimators'])\n",
    "etc.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "predictions = etc.predict(x_validation_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and index for the confusion matrix\n",
    "cmat = confusion_matrix(y_val, predictions)\n",
    "cmat = pd.DataFrame(cmat, index=['Denial of Service', 'Malware', 'None', 'Port Scan'], columns=['Denial of Service', 'Malware', 'None', 'Port Scan'])\n",
    "\n",
    "# Use seaborn to visualize the confusion matrix\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cmat, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.5. Extra Trees model predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "predictions = etc.predict(x_test_scaled)\n",
    "\n",
    "# Show the predictions on a histogram\n",
    "fig = sns.countplot(x=predictions)\n",
    "fig.set_title('Predictions distribution on the test set') # Set the title\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=45) # Rotate x-labels\n",
    "pd.Series(predictions).value_counts() # Print the predictions size per class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. Neural Network classifier training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6.1. Grid search to find best hyperparameters for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLPClasifier\n",
    "mlp = MLPClassifier(\n",
    "  max_iter=1000,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "# Grid search for MLPClassifier\n",
    "parameters = {\n",
    "  'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "  'activation': ['relu', 'tanh'],\n",
    "  'alpha': [0.0001, 0.001],\n",
    "  'solver': ['adam', 'lbfgs'],\n",
    "  'learning_rate': ['constant', 'invscaling'],\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(\n",
    "  mlp,\n",
    "  parameters,\n",
    "  cv=2, # Only 2 folds because of the size of the dataset, otherwise it takes too long\n",
    "  n_jobs=-1, # Use all cores\n",
    ")\n",
    "\n",
    "mlp_grid.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best params\n",
    "print(f\"Best params: {mlp_grid.best_params_}\")\n",
    "print(f\"Best score: {mlp_grid.best_score_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6.2. Create model with best parameters + fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLPClassifier with best parameters\n",
    "mlp = MLPClassifier(\n",
    "  hidden_layer_sizes=mlp_grid.best_params_['hidden_layer_sizes'],\n",
    "  activation=mlp_grid.best_params_['activation'],\n",
    "  alpha=mlp_grid.best_params_['alpha'],\n",
    "  solver=mlp_grid.best_params_['solver'],\n",
    "  learning_rate=mlp_grid.best_params_['learning_rate'],\n",
    "  max_iter=1000,\n",
    "  random_state=42\n",
    ")\n",
    "mlp.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6.3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "predictions = mlp.predict(x_validation_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6.4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and index for the confusion matrix\n",
    "cmat = confusion_matrix(y_val, predictions)\n",
    "cmat = pd.DataFrame(cmat, index=['Denial of Service', 'Malware', 'None', 'Port Scan'], columns=['Denial of Service', 'Malware', 'None', 'Port Scan'])\n",
    "\n",
    "# Use seaborn to visualize the confusion matrix\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cmat, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6.5. MPL classifier model predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "predictions = mlp.predict(x_test_scaled)\n",
    "\n",
    "# Show the predictions on a histogram\n",
    "fig = sns.countplot(x=predictions)\n",
    "fig.set_title('Predictions distribution on the test set') # Set the title\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=45) # Rotate x-labels\n",
    "pd.Series(predictions).value_counts() # Print the predictions size per class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparative Analysis of Classification Models\n",
    "\n",
    "I have observed outstanding performance across all tested classification models. It is worth noting that nearly every model exhibited exceptional accuracy, as evidenced by an f1-score nearing perfection (1.0).\n",
    "\n",
    "Upon thorough consideration, I have come to realize that this phenomenon stems from the substantial resemblance between the utilized validation set (created by splitting the training set, given the absence of the target variable in the provided test set) and the training set employed for model training.\n",
    "\n",
    "Presumably, the training set was constructed using data from a specific network, resulting in a significant overlap of features between both sets. Consequently, the models achieved highly accurate classifications for almost all flows within the validation set. However, their performance may not be equally robust when applied to a distinct test set derived from a different network.\n",
    "\n",
    "Nevertheless, I have opted to present the classification outcomes of each model on the provided test set, despite the unavailability of performance metrics for evaluation.\n",
    "\n",
    "Acquiring a test set originating from a diverse network would have been advantageous, enabling a more precise assessment of the models' performance. Regrettably, obtaining such a test set proved unfeasible for this particular project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
